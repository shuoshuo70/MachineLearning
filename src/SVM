SVM: support vector machines 支持向量机
思想：支持向量机试图通过求解一个二次优化问题来最大化分类间隔。引入SMO算法通过每次只优化2个alpha
值来加快SVM的训练速度。核方法将数据从一个低维空间映射到一个高维空间，可以将一个低维空间的非线性
问题转化为高维空间的线性问题求解，其中径向基函数是一个常用的度量两个向量距离的核函数


SVM --- 分类器(二分类，当解决多分类时需要额外的方法扩展)


名词解释：
1. 间隔：点到分割超平面的距离
2. 支持向量：离分隔超平面最近的那些点

我们希望间隔越大，这样分类器的效果越好
分类器的输出标签为+1，-1，标签*距离，值越大代表离分界线越远
由此我们获得一个最小优化函数，用SMO算法求解


SMO算法工作原理：
   每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么久增大其中一个同时减小
   另一个。这里的合适指的是：1. 这两个alpha必须要在间隔边界之外， 2. 还没进行区间化处理或
   不在边界上


SMO算法是线性分类，那对于非线性的如何处理呢？ --> 核函数 ：将数据转换成易于分类器理解的形式，即
一个特征空间到另一个特征空间的映射。
在高维空间解决线性问题，等价于在低维空间解决非线性问题


支持向量的数目存在一个最优值。如果支持向量太少，可能会得到一个很差的决策边界；太多，相当于每次都
利用整个数据集进行分类，退化为KNN


SVM 与 KNN的区别：
KNN需要保留所有的训练样本，而SVM只保留支持向量，占用空间少


优点：
计算开销小
缺点：
二分类，多分类时需要加额外的方法


